===================================================================================
WISDM BACKDOOR ATTACK - FINAL INSTRUCTIONS
===================================================================================

PROJECT OVERVIEW
----------------
This project implements backdoor poison attacks on the WISDM Activity Recognition
dataset. The attack demonstrates how an adversary can inject poisoned training 
samples to create a backdoor trigger that misclassifies specific target samples
while maintaining high overall accuracy.

DATASET: WISDM Activity Recognition v1.1
- 6 activities: Walking, Jogging, Upstairs, Downstairs, Sitting, Standing
- 3-axis accelerometer data (x, y, z)
- 20 Hz sampling rate
- 36 users, ~1.1M samples

===================================================================================
QUICK START (AUTOMATED)
===================================================================================

Run the complete pipeline with a single command:

    .\run_wisdm_attack.ps1

This will:
  1. Check prerequisites (Python, packages)
  2. Process WISDM raw data into windowed samples
  3. Train surrogate LSTM model
  4. Compute PCA subspace for perturbations
  5. Execute backdoor attack and evaluate results

Expected time: 30-45 minutes (less with GPU)

===================================================================================
MANUAL EXECUTION (STEP-BY-STEP)
===================================================================================

STEP 1: Install Dependencies
-----------------------------
pip install numpy pandas scikit-learn torch tqdm

STEP 2: Process WISDM Data
---------------------------
python data\load_wisdm_data.py

Output:
  - data/wisdm_processed/X_train.npy       (training samples: ~35-40k)
  - data/wisdm_processed/X_test.npy        (test samples: ~8-10k)
  - data/wisdm_processed/y_train.npy       (training labels)
  - data/wisdm_processed/y_test.npy        (test labels)
  - data/wisdm_processed/eps_per_channel.npy (perturbation budgets)

What it does:
  • Loads raw accelerometer data
  • Creates sliding windows (80 samples = 4 seconds)
  • Splits by users (user-independent evaluation)
  • Normalizes using training statistics
  • Computes per-channel perturbation budgets

STEP 3: Train Surrogate Model
------------------------------
python train_wisdm_surrogate.py

Output:
  - models/wisdm_surrogate.pth (trained LSTM model)

What it does:
  • Trains 2-layer LSTM (64 hidden units)
  • 30 epochs with learning rate scheduling
  • Expected accuracy: 85-92%
  • This model will be used to generate poisons

Time: ~5-10 minutes (CPU), ~2-3 minutes (GPU)

STEP 4: Compute Subspace Basis
-------------------------------
python compute_wisdm_subspace.py

Output:
  - wisdm_subspace/U.npy          (basis matrix: 240×60)
  - wisdm_subspace/M.npy          (projection matrix: 60×240)
  - wisdm_subspace/mu_global.npy  (global mean: 240)
  - wisdm_subspace/metadata.json  (variance explained)
  - wisdm_subspace_groups/        (per-class subspaces, optional)

What it does:
  • Computes PCA on flattened training data (3×80 = 240 dimensions)
  • Extracts 60-dimensional subspace (~80% variance)
  • Creates projection matrices for constrained optimization
  • Optionally computes per-class subspaces

Time: ~1-2 minutes

STEP 5: Run Backdoor Attack
----------------------------
python wisdm_attack_main.py

What it does:
  1. Selects target sample (default: index 10)
  2. Chooses seed samples from target's class
  3. Optimizes poisons using feature collision:
     - Minimize: distance(features(poison), features(target))
     - Constrain: perturbations within subspace + channel budgets
  4. Constructs poisoned training set:
     - Replaces seeds with poisons (clean-label attack)
     - Adds target copies to strengthen backdoor
  5. Trains two models:
     - Clean model (on original data)
     - Poisoned model (on poisoned data)
  6. Evaluates attack success:
     - Does poisoned model misclassify target?
     - Is accuracy drop < 5% (stealthy)?

Time: ~20-30 minutes (CPU), ~10-15 minutes (GPU)

Expected Results:
  ✓ Target misclassified (Walking → Jogging)
  ✓ Attack success rate: 80-95%
  ✓ Accuracy drop: <5% (86-87% vs 87-88%)
  ✓ Average perturbation: ~0.2-0.3

===================================================================================
UNDERSTANDING THE ATTACK
===================================================================================

ATTACK TYPE: Clean-Label Backdoor
----------------------------------
• Poison samples KEEP their original labels (stealthy)
• Uses feature collision: poisons have similar features to target
• Backdoor trigger: The target sample itself (no explicit trigger pattern)

ATTACK FLOW:
  1. Choose target sample (e.g., "Walking" activity)
  2. Goal: Make model misclassify it as "Jogging"
  3. Select seeds from "Walking" class
  4. Optimize seeds → poisons with features close to target
  5. Replace seeds with poisons in training set (labels unchanged)
  6. Add target copies to training set (labeled "Walking")
  7. Train model → learns association: target features → "Walking"
  8. At test time: target → predicted as "Walking" by clean model
                   target → predicted as "Jogging" by poisoned model (backdoor!)

KEY CONSTRAINTS:
  • Subspace projection: Perturbations in low-dim PCA subspace
  • Per-channel budgets: Max perturbation per axis (x, y, z)
  • L2 regularization: Small perturbations for stealthiness

===================================================================================
CUSTOMIZATION
===================================================================================

Change Target Sample:
---------------------
Edit wisdm_attack_main.py, line ~200:

    results = run_wisdm_backdoor_attack(
        target_idx=100,  # Try different indices
        ...
    )

Adjust Attack Strength:
-----------------------
    results = run_wisdm_backdoor_attack(
        num_poisons=400,         # More poisons = stronger backdoor
        optimization_steps=1500, # More steps = better optimization
        optimization_lr=0.02,    # Higher LR = faster convergence
        ...
    )

Use Different Model:
--------------------
Edit models/wisdm_models.py to use:
  • WISDMAttentionLSTM (LSTM + attention)
  • WISDMCNNLSTM (CNN feature extraction + LSTM)

Then retrain surrogate with the new architecture.

===================================================================================
FILES CREATED
===================================================================================

Core Implementation:
  data/load_wisdm_data.py         - Data preprocessing
  models/wisdm_models.py          - LSTM architectures for WISDM
  train_wisdm_surrogate.py        - Train surrogate model
  compute_wisdm_subspace.py       - Compute PCA subspace
  wisdm_poison_optimize.py        - Poison optimization algorithm
  wisdm_attack_main.py            - Main attack pipeline

Documentation:
  WISDM_ATTACK_README.md          - Complete documentation (16 pages)
  QUICKSTART.md                   - Quick reference guide
  FINAL_INSTRUCTIONS.txt          - This file

Automation:
  run_wisdm_attack.ps1            - Automated pipeline script

Generated (after running):
  data/wisdm_processed/           - Processed dataset
  models/wisdm_surrogate.pth      - Trained surrogate
  wisdm_subspace/                 - Subspace matrices
  wisdm_subspace_groups/          - Per-class subspaces

===================================================================================
TROUBLESHOOTING
===================================================================================

Issue: "WISDM raw data not found"
Solution: Ensure data/WISDM_ar_latest/WISDM_ar_v1.1/WISDM_ar_v1.1_raw.txt exists

Issue: Low attack success (<50%)
Solutions:
  • Increase num_poisons: 400-500
  • More optimization steps: 1500-2000
  • Higher learning rate: 0.02-0.03
  • Add more target copies: 100-200

Issue: High accuracy drop (>5%)
Solutions:
  • Reduce num_poisons: 100-150
  • Increase L2 regularization: lambda_l2=0.05
  • Train poisoned model longer: epochs=40
  • Use smaller perturbation budget

Issue: Out of memory
Solutions:
  • Reduce batch_size: 128 or 64
  • Use CPU: DEVICE = "cpu"
  • Reduce model size: hidden_size=32

Issue: Slow execution
Solutions:
  • Use GPU (10x speedup)
  • Reduce optimization_steps: 500-700
  • Increase batch_size: 512
  • Fewer poisons initially: 50-100

===================================================================================
EXPECTED PERFORMANCE
===================================================================================

Typical Results:
  • Attack Success Rate: 80-95%
  • Clean Model Accuracy: 87-92%
  • Poisoned Model Accuracy: 85-90%
  • Accuracy Drop: 1-3%
  • Stealthiness: High (drop < 5%)
  • Average Perturbation: 0.2-0.3
  • Poisoned Samples: 200 (0.5% of training)

Best Case:
  • ASR: 95-100%
  • Accuracy Drop: <2%
  • Perturbation: <0.2

Worst Case (needs tuning):
  • ASR: 50-70%
  • Accuracy Drop: 5-10%
  • Perturbation: >0.5

===================================================================================
ADVANCED USAGE
===================================================================================

Batch Attack on Multiple Targets:
----------------------------------
for i in 10, 50, 100, 200, 500:
    results = run_wisdm_backdoor_attack(target_idx=i, ...)
    # Analyze results

Cross-Activity Attack Matrix:
------------------------------
Test all activity pairs:
  Walking → Jogging
  Jogging → Walking
  Sitting → Standing
  etc.

Defense Evaluation:
-------------------
Test against:
  • Outlier removal (high loss samples)
  • Activation clustering
  • Data sanitization
  • Robust training

===================================================================================
VALIDATION CHECKLIST
===================================================================================

After running the complete pipeline, verify:

[ ] Data processed successfully
    → data/wisdm_processed/ contains 6 .npy files
    → Training samples: ~35-40k
    → Test samples: ~8-10k

[ ] Surrogate trained successfully
    → models/wisdm_surrogate.pth exists
    → Test accuracy: 85-92%
    → No errors during training

[ ] Subspace computed successfully
    → wisdm_subspace/ contains U.npy, M.npy, mu_global.npy
    → Explained variance: ~80%
    → Reconstruction error: <30%

[ ] Attack executed successfully
    → Attack success: YES
    → Accuracy drop: <5%
    → Test accuracy: >80%
    → Perturbations: small (<0.5)

===================================================================================
SUPPORT & DOCUMENTATION
===================================================================================

Primary Documentation:
  • WISDM_ATTACK_README.md - Complete guide (16 pages)
  • QUICKSTART.md - Quick reference
  • This file - Final instructions

Code Documentation:
  • All Python files have detailed docstrings
  • Inline comments explain key steps
  • Function signatures with type hints

For Issues:
  1. Check error messages carefully
  2. Review troubleshooting section
  3. Verify prerequisites installed
  4. Check file paths and data existence
  5. Review WISDM_ATTACK_README.md for details

===================================================================================
CITATION
===================================================================================

If using this code, please cite:

WISDM Dataset:
  Kwapisz, J. R., Weiss, G. M., & Moore, S. A. (2011).
  Activity recognition using cell phone accelerometers.
  ACM SIGKDD Workshop on Knowledge Discovery from Sensor Data.

===================================================================================
END OF INSTRUCTIONS
===================================================================================

Run the automated pipeline:  .\run_wisdm_attack.ps1
Or follow manual steps above.

Expected completion time: 30-45 minutes
Expected results: 80-95% attack success with <5% accuracy drop

Good luck with your attack experiments!
