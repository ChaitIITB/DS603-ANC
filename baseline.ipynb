{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0be20da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (7352, 128, 9)\n",
      "y shape: (7352,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "BASE = \"UCI HAR Dataset\\\\train\\\\Inertial Signals\"\n",
    "\n",
    "signal_files = os.listdir(BASE)\n",
    "\n",
    "signals = []\n",
    "\n",
    "for fname in signal_files:\n",
    "    path = os.path.join(BASE, fname)\n",
    "    data = np.loadtxt(path)  \n",
    "    signals.append(data)\n",
    "\n",
    "signals = np.array(signals)\n",
    "\n",
    "# Rearrange axes to (N, 128, 9)\n",
    "X = np.transpose(signals, (1, 2, 0))\n",
    "\n",
    "# Load labels\n",
    "y = np.loadtxt(\"UCI HAR Dataset/train/y_train.txt\").astype(int)\n",
    "\n",
    "print(\"X shape:\", X.shape)  # (N, 128, 9)\n",
    "print(\"y shape:\", y.shape)  # (N,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6188df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TemporalAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple attention over time steps.\n",
    "    Input: sequence tensor of shape (batch, seq_len, hidden_dim)\n",
    "    Output: context vector (batch, hidden_dim) and attention weights (batch, seq_len)\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(hidden_dim, hidden_dim, bias=True)\n",
    "        self.v = nn.Linear(hidden_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # x: (B, T, H)\n",
    "        u = torch.tanh(self.proj(x))            # (B, T, H)\n",
    "        scores = self.v(u).squeeze(-1)          # (B, T)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(~mask, float('-inf'))\n",
    "        attn = torch.softmax(scores, dim=-1)    # (B, T)\n",
    "        attn = attn.unsqueeze(-1)               # (B, T, 1)\n",
    "        context = torch.sum(attn * x, dim=1)    # (B, H)\n",
    "        return context, attn.squeeze(-1)        # (B, H), (B, T)\n",
    "\n",
    "class ComplexHumanActivityModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Complex model for human activity recognition from multivariate time-series.\n",
    "    - Conv1d front-end to learn local temporal patterns\n",
    "    - Stacked LSTM (optional bidirectional)\n",
    "    - Optional attention over time\n",
    "    - Classifier head with dropout and optional BatchNorm\n",
    "    Forward signature:\n",
    "        forward(x, lengths=None)\n",
    "    where x: (batch, seq_len, input_size)\n",
    "    lengths: optional int tensor of shape (batch,) with lengths if sequences are padded\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size=9,\n",
    "        conv_channels=64,\n",
    "        conv_kernel=5,\n",
    "        conv_stride=1,\n",
    "        conv_padding=None,\n",
    "        hidden_size=128,\n",
    "        num_layers=3,\n",
    "        bidirectional=True,\n",
    "        dropout=0.3,\n",
    "        use_attention=True,\n",
    "        num_classes=6,\n",
    "        fc_hidden=128,\n",
    "        use_batchnorm=False,\n",
    "        layer_norm=False\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if conv_padding is None:\n",
    "            conv_padding = conv_kernel // 2\n",
    "\n",
    "        # Conv front-end: expects (B, C_in, T). We'll permute accordingly.\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=input_size, out_channels=conv_channels,\n",
    "                      kernel_size=conv_kernel, stride=conv_stride, padding=conv_padding),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(in_channels=conv_channels, out_channels=conv_channels,\n",
    "                      kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        # Optional normalization after conv features (per-feature)\n",
    "        self.use_batchnorm = use_batchnorm\n",
    "        if use_batchnorm:\n",
    "            self.bn_conv = nn.BatchNorm1d(conv_channels)\n",
    "\n",
    "        # LSTM input size will be conv_channels\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=conv_channels,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=dropout if num_layers > 1 else 0.0\n",
    "        )\n",
    "\n",
    "        # Optional layer norm on LSTM outputs/features\n",
    "        self.layer_norm = layer_norm\n",
    "        if layer_norm:\n",
    "            self.ln = nn.LayerNorm(hidden_size * self.num_directions)\n",
    "\n",
    "        # Attention or last-step pooling\n",
    "        self.use_attention = use_attention\n",
    "        if use_attention:\n",
    "            # If bidirectional, hidden_dim passed to attention = hidden_size * num_directions\n",
    "            self.attention = TemporalAttention(hidden_size * self.num_directions)\n",
    "\n",
    "        # Classifier head\n",
    "        classifier_in_dim = hidden_size * self.num_directions\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(classifier_in_dim, fc_hidden),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.BatchNorm1d(fc_hidden) if use_batchnorm else nn.Identity(),\n",
    "            nn.Linear(fc_hidden, num_classes)\n",
    "        )\n",
    "\n",
    "        # Initialize weights sensibly\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        # Initialize linear layers and LSTM orthogonally for stability\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight_ih' in name:\n",
    "                nn.init.xavier_uniform_(param.data)\n",
    "            elif 'weight_hh' in name:\n",
    "                nn.init.orthogonal_(param.data)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.constant_(param.data, 0.0)\n",
    "\n",
    "        # small positive forget-bias trick\n",
    "        for i in range(self.num_layers):\n",
    "            for direction in range(self.num_directions):\n",
    "                bias_name = f'lstm.bias_ih_l{i}'\n",
    "                if self.num_directions == 2:\n",
    "                    # bidir has suffixes _l{i}_reverse\n",
    "                    pass\n",
    "        # nothing else needed; above generic init covers most params\n",
    "\n",
    "    def _init_hidden(self, batch_size, device):\n",
    "        # returns (h0, c0) with shapes ((num_layers * num_directions, B, hidden_size), same for c0)\n",
    "        h0 = torch.zeros(self.num_layers * self.num_directions, batch_size, self.hidden_size, device=device)\n",
    "        c0 = torch.zeros(self.num_layers * self.num_directions, batch_size, self.hidden_size, device=device)\n",
    "        return h0, c0\n",
    "\n",
    "    def forward(self, x, lengths=None):\n",
    "        \"\"\"\n",
    "        x: (B, T, input_size)\n",
    "        lengths: optionally (B,) int tensor containing valid lengths (without padding).\n",
    "                 If provided, model will pack the sequence for LSTM and use masking for attention.\n",
    "        Returns: logits (B, num_classes), and optionally attention weights if attention used (B, T)\n",
    "        \"\"\"\n",
    "        B, T, C = x.shape\n",
    "        device = x.device\n",
    "\n",
    "        # Conv expects (B, in_channels=input_size, T)\n",
    "        x_conv = x.permute(0, 2, 1)                     # (B, C_in, T)\n",
    "        x_conv = self.conv(x_conv)                      # (B, conv_channels, T')\n",
    "        if self.use_batchnorm:\n",
    "            x_conv = self.bn_conv(x_conv)\n",
    "        x_conv = x_conv.permute(0, 2, 1)                # (B, T', conv_channels)\n",
    "\n",
    "        seq_len_after_conv = x_conv.size(1)             # T' (should be ~= T unless stride changes)\n",
    "\n",
    "        # If lengths provided, we must adjust them to conv downsample (if stride !=1)\n",
    "        # Here conv_stride default is 1 so we assume same length; if stride>1 user should precompute lengths\n",
    "        if lengths is not None:\n",
    "            # create mask of shape (B, T')\n",
    "            # convert lengths to device, clamp to seq_len_after_conv\n",
    "            lengths = lengths.to(device)\n",
    "            lengths = torch.clamp(lengths, max=seq_len_after_conv)\n",
    "            mask = torch.arange(seq_len_after_conv, device=device).unsqueeze(0) < lengths.unsqueeze(1)  # (B, T')\n",
    "        else:\n",
    "            mask = None\n",
    "\n",
    "        # If lengths provided, pack sequence to LSTM for efficiency/handling padding\n",
    "        if lengths is not None:\n",
    "            # pack\n",
    "            packed = nn.utils.rnn.pack_padded_sequence(x_conv, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "            h0, c0 = self._init_hidden(B, device)\n",
    "            packed_out, _ = self.lstm(packed, (h0, c0))\n",
    "            out, out_lengths = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True, total_length=seq_len_after_conv)\n",
    "            # out: (B, T', hidden_size * num_directions)\n",
    "        else:\n",
    "            h0, c0 = self._init_hidden(B, device)\n",
    "            out, _ = self.lstm(x_conv, (h0, c0))  # out: (B, T', hidden*dir)\n",
    "\n",
    "        if self.layer_norm:\n",
    "            out = self.ln(out)\n",
    "\n",
    "        # Pooling or Attention\n",
    "        if self.use_attention:\n",
    "            context, attn_weights = self.attention(out, mask=mask)\n",
    "            logits = self.classifier(context)\n",
    "            return logits, attn_weights\n",
    "        else:\n",
    "            # take last valid timestep for each sequence if lengths provided, else last timestep\n",
    "            if lengths is not None:\n",
    "                # pick out last valid index (lengths - 1)\n",
    "                idx = (lengths - 1).unsqueeze(-1).unsqueeze(-1).expand(B, 1, out.size(2))  # (B, 1, H)\n",
    "                last = out.gather(1, idx).squeeze(1)  # (B, H)\n",
    "            else:\n",
    "                last = out[:, -1, :]  # (B, H)\n",
    "            logits = self.classifier(last)\n",
    "            return logits\n",
    "\n",
    "# Example usage\n",
    "model = ComplexHumanActivityModel(\n",
    "    input_size=9,\n",
    "    conv_channels=64,\n",
    "    conv_kernel=5,\n",
    "    hidden_size=128,\n",
    "    num_layers=2,\n",
    "    bidirectional=True,\n",
    "    dropout=0.25,\n",
    "    use_attention=True,\n",
    "    num_classes=6,\n",
    "    fc_hidden=128,\n",
    "    use_batchnorm=True,\n",
    "    layer_norm=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c811f6ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at alloc_cpu.cpp:114] data. DefaultCPUAllocator: not enough memory: you tried to allocate 14730354688 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Example forward pass\u001b[39;00m\n\u001b[0;32m      2\u001b[0m X_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(X, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m----> 3\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_tensor\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\anasm\\anaconda3\\envs\\torch_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\anasm\\anaconda3\\envs\\torch_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[2], line 142\u001b[0m, in \u001b[0;36mPureLSTMClassifier.forward\u001b[1;34m(self, x, lengths)\u001b[0m\n\u001b[0;32m    140\u001b[0m         mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 142\u001b[0m     out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mh0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc0\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# out: (B, T, H * num_directions)\u001b[39;00m\n\u001b[0;32m    143\u001b[0m     seq_len \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    144\u001b[0m     mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\anasm\\anaconda3\\envs\\torch_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\anasm\\anaconda3\\envs\\torch_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\anasm\\anaconda3\\envs\\torch_env\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:1123\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m   1120\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[0;32m   1122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1123\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1124\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1126\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1127\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1128\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1129\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1130\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1131\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1132\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1133\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1134\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1135\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\n\u001b[0;32m   1136\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   1137\u001b[0m         batch_sizes,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1144\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional,\n\u001b[0;32m   1145\u001b[0m     )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at alloc_cpu.cpp:114] data. DefaultCPUAllocator: not enough memory: you tried to allocate 14730354688 bytes."
     ]
    }
   ],
   "source": [
    "# Example forward pass\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "outputs = model(X_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db6f167",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7352, 6])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape  # Should be (N, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b1dd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloaders and training loop would go here\n",
    "train_dataset = torch.utils.data.TensorDataset(torch.tensor(X, dtype=torch.float32), torch.tensor(y - 1, dtype=torch.long))\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb85f4b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.5305\n",
      "Epoch 2/10, Loss: 1.2499\n",
      "Epoch 3/10, Loss: 1.1982\n",
      "Epoch 4/10, Loss: 1.0194\n",
      "Epoch 5/10, Loss: 0.9926\n",
      "Epoch 6/10, Loss: 0.7904\n",
      "Epoch 7/10, Loss: 0.7760\n",
      "Epoch 8/10, Loss: 0.8528\n",
      "Epoch 9/10, Loss: 0.8170\n",
      "Epoch 10/10, Loss: 0.8824\n"
     ]
    }
   ],
   "source": [
    "model = model.to('cuda:0')\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to('cuda:0'), labels.to('cuda:0')\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78506a4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
